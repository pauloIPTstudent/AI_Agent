# Maestro AI Agent - Backend (FastAPI + Ollama)
Este Ã© o servidor backend do projeto Maestro, responsÃ¡vel por gerenciar a lÃ³gica do Agente de IA, integraÃ§Ã£o com LangGraph e comunicaÃ§Ã£o com a LLM local via Ollama.

## ğŸ› ï¸ PrÃ©-requisitos
Antes de comeÃ§ar, certifique-se de ter instalado em sua mÃ¡quina Linux:

Python 3.10 ou superior

Ollama (para rodar a LLM localmente)

Pip (gerenciador de pacotes Python)

## ğŸ“¦ DependÃªncias Principais
As bibliotecas essenciais utilizadas neste projeto sÃ£o:

FastAPI: Framework web de alta performance.

Uvicorn: Servidor ASGI para rodar a aplicaÃ§Ã£o.

Ollama: Biblioteca para integraÃ§Ã£o com o motor de IA local.

Pydantic: ValidaÃ§Ã£o de dados e esquemas.

## ğŸ“¥ InstalaÃ§Ã£o
Siga os passos abaixo para configurar o ambiente:

Acesse a pasta do backend:

Bash
cd ~/AI_Agent/backend
Crie um ambiente virtual (VENV):

Bash
python3 -m venv venv
Ative o ambiente virtual:

Bash
source venv/bin/activate
Instale as dependÃªncias:

Bash
pip install -r requirements.txt

## ğŸ§  ConfiguraÃ§Ã£o da IA (Ollama)
O backend depende do Ollama rodando no servidor.

Certifique-se de que o Ollama estÃ¡ ativo:

Bash
ollama serve
Baixe o modelo utilizado (ex: Llama 3):

Bash
ollama pull llama3
ğŸš€ Como Rodar o Servidor
Com o ambiente virtual ativado, execute:

Bash
python3 main.py
O servidor iniciarÃ¡ por padrÃ£o em: http://localhost:8000

ğŸ”Œ Endpoints Principais
POST /chat: Recebe uma mensagem do usuÃ¡rio e retorna a resposta da LLM.


Dica para ProduÃ§Ã£o (Ngrok)
Para expor o backend para o frontend atravÃ©s de um tÃºnel seguro:

Bash
ngrok http 8000